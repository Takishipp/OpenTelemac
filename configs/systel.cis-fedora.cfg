# _____                              _______________________________
# ____/ TELEMAC Project Definitions /______________________________/
#
[Configurations]
configs:    fedgfortrans fedgfortransdbg fedgfortranp fedgfortranpdbg
#
#  fedgfopenmpi fedgfopenmpidbg
#
# _____          ___________________________________________________
# ____/ GENERAL /__________________________________________________/
[general]
modules:    system -dredgesim
#
cmd_lib:    ar cru <libname> <objs>
#
mods_all:   -I <config>
#
sfx_zip:    .gztar
sfx_lib:    .a
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:
#
val_root:   <root>/examples
val_rank:   all
# also possible val_rank:   <3 >7 6
#
# _____                         ____________________________________
# ____/ Fedora gfortran scalar /___________________________________/
[fedgfortrans]
#
brief: scalar mode, Fortran optimisation 3.
   TELEMAC will work whether processor is 0 or 1
#
cmd_obj:    gfortran -c -O3 -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    gfortran -fconvert=big-endian -frecord-marker=4 -v -lm -lz -o <exename> <objs> <libs>
#
# _____                               ____________________________________
# ____/ Fedora gfortran scalar debug /___________________________________/
[fedgfortransdbg]
#
brief: scalar mode, Fortran debug mode.
   TELEMAC will work whether processor is 0 or 1
#
cmd_obj:    gfortran -c -g -fbounds-check -Wall -fbacktrace -finit-real=nan -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    gfortran -fconvert=big-endian -frecord-marker=4 -v -lm -lz -o <exename> <objs> <libs>
#
# _____                          ___________________________________
# ____/ Fedora gfortran openMPI /__________________________________/
[fedgfopenmpi]
#
brief: parallel mode, using mpiexec directly (of the openMPI package).
       The only difference with the scalar versions (optimised) is the presence
       of the key mpi_cmdexec and the -DHAVE_MPI compilation directive.
       Of course, you also need the key par_cmdexec.
       Finally, note that this configuration also works whether
       processor is 0 or 1.
#
mpi_cmdexec:   /usr/lib64/openmpi/bin/mpiexec -wdir <wdir> -n <ncsize> <exename>
#
cmd_obj:    gfortran -c -O3 -DHAVE_MPI -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    /usr/lib64/openmpi/bin/mpif90 -fconvert=big-endian -frecord-marker=4 -lpthread -v -lm -lz -o <exename> <objs> <libs>
#
incs_special:       -I /usr/include/openmpi-x86_64
incs_parallel:      -I /usr/include/openmpi-x86_64
libs_partel:        /home/telemac/metis-5.1.0/build/Linux-x86_64/libmetis/libmetis.a
libs_all:           /usr/lib64/openmpi/lib/libmpi.so
#
# _____                                ___________________________________
# ____/ Fedora gfortran openMPI debug /__________________________________/
[fedgfopenmpidbg]
#
brief: parallel mode, using mpiexec directly (of the openMPI package).
       The only difference with the scalar versions (debugged) is the presence
       of the key mpi_cmdexec and the -DHAVE_MPI compilation directive.
       Of course, you also need the key par_cmdexec.
       Finally, note that this configuration also works whether
       processor is 0 or 1.
#
mpi_cmdexec:   /usr/lib64/openmpi/bin/mpiexec -wdir <wdir> -n <ncsize> <exename>
#
cmd_obj:    gfortran -c -g -DHAVE_MPI -fbounds-check -Wall -fbacktrace -finit-real=nan -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    /usr/lib64/openmpi/bin/mpif90 -fconvert=big-endian -frecord-marker=4 -lpthread -v -lm -o <exename> <objs> <libs>
#
incs_special:       -I /usr/include/openmpi-x86_64
incs_parallel:      -I /usr/include/openmpi-x86_64
libs_partel:        /home/telemac/metis-5.1.0/build/Linux-x86_64/libmetis/libmetis.a
libs_all       :    /usr/lib64/openmpi/lib/libmpi.so
#
# _____                         ____________________________________
# ____/ Fedora gfortran MPICH2 /___________________________________/
[fedgfortranp]
#
brief: parallel mode, using mpiexec directly (of the MPICH2 package).
       The only difference with the scalar versions (debugged) is the presence
       of the key mpi_cmdexec and the -DHAVE_MPI compilation directive.
       Of course, you also need the key par_cmdexec.
       Finally, note that this configuration also works whether
       processor is 0 or 1.
#
mpi_cmdexec:   /usr/lib64/mpich/bin/mpiexec -wdir <wdir> -n <ncsize> <exename>
#
cmd_obj:    gfortran -c -O3 -DHAVE_MPI -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    /usr/lib64/mpich/bin/mpif90 -fconvert=big-endian -frecord-marker=4 -lpthread -v -lm -o <exename> <objs> <libs>
#
incs_special:       -I /usr/include/mpich-x86_64
incs_parallel:      -I /usr/include/mpich-x86_64
libs_partel:        /home/telemac/metis-5.1.0/build/Linux-x86_64/libmetis/libmetis.a
libs_all:           /usr/lib64/mpich/lib/libmpichf90.so.10
#
#
#_____                               ____________________________________
# ____/ Fedora gfortran MPICH2 debug/___________________________________/
[fedgfortranpdbg]
#
brief: parallel mode, using mpiexec directly (of the MPCHI2 package).
       The only difference with the scalar versions (debugged) is the presence
       of the key mpi_cmdexec and the -DHAVE_MPI compilation directive.
       Of course, you also need the key par_cmdexec.
       Finally, note that this configuration also works whether
       processor is 0 or 1.
#
mpi_cmdexec:   /usr/lib64/mpich/bin/mpiexec -wdir <wdir> -n <ncsize> <exename>
#
cmd_obj:    gfortran -c -g -DHAVE_MPI -fbounds-check -Wall -fbacktrace -finit-real=nan -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    /usr/lib64/mpich/bin/mpif90 -fconvert=big-endian -frecord-marker=4 -lpthread -v -lm -o <exename> <objs> <libs>
#
incs_special:       -I /usr/include/mpich-x86_64
incs_parallel:      -I /usr/include/mpich-x86_64
libs_partel:        /home/telemac/metis-5.1.0/build/Linux-x86_64/libmetis/libmetis.a
libs_all:           /usr/lib64/mpich/lib/libmpichf90.so.10
