# _____                              _______________________________
# ____/ TELEMAC Project Definitions /______________________________/
#
[Configurations]
configs:   C7.gfortran C7.pgi11 C7.ifort10 C7.gfortranHPC C7.ifort10HPC C7.nag5 C7.nag5HPC
# _____                        ____________________________________
# ____/ Generalr /___________________________________/
# Global declaration that are true for all the C7.* configuration
[general]
root:       /netdata/systel/V6P2
version:    v6p2
language:   2
modules:    update system
options:    
#
sfx_zip:    .zip
sfx_lib:    .a
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:    
#
#val_root:   <root>/../validation
val_root:   <root>/examples
# also possible val_root:   <modpath>/calibration
val_rank:   all
#val_rank: =11
# also possible val_rank:   <3 >7 6
#
# _____                        ____________________________________
# ____/ Calibre7 Intel Scalar /___________________________________/
[C7.ifort10]
#
cmd_obj:    ifort -c -O3 -convert big_endian -DHAVE_VTK -DHAVE_TECPLOT <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    ifort -convert big_endian -o <exename> <objs> -Xlinker --start-group <libs> --end-group
cmd_exe_estel3d:    ifort -convert big_endian -o <exename> <objs> -Xlinker --start-group <libs> --end-group -lstdc++ -lz
#
mods_all:   -I <config>
libs_estel3d:/netdata/systel/LIBRARY/tecplot/tecplot_10/intel_64_10_C7/tecplot10.a
#MED libs_all: -L/netdata/systel/LIBRARY/MED/MED-3.0.5_intel_64_10_C7/lib -lmed -L/netdata/systel/LIBRARY/HDF5/HDF5-1.8.7_intel_64_10_C7/lib -lhdf5

#
# _____                        ____________________________________
# ____/ Calibre7 PGI Scalar /_____________________________________/
[C7.pgi11]
#
cmd_obj:    pgf90  -c -O3 -byteswapio -DHAVE_VTK -DHAVE_TECPLOT <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    pgf90  -c -O3 -byteswapio -o <exename> <objs> -Xlinker --start-group <libs> --end-group
#
mods_all:   -I <config>
libs_estel3d:/netdata/systel/LIBRARY/tecplot/tecplot_10/intel_64_10_C7/tecplot10.a

# __                       	       _____________________________________
# ____/ Calibre7 GFORTRAN 4.4.5 Scalar /_______________________________/
[C7.gfortran]
#
cmd_obj:    gfortran -c -O3 -fconvert=big-endian -frecord-marker=4 -DHAVE_VTK -DHAVE_TECPLOT <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    gfortran -fconvert=big-endian -frecord-marker=4 -o <exename> <objs> -Xlinker --start-group <libs> --end-group
cmd_exe_estel3d:    gfortran -fconvert=big-endian -frecord-marker=4 -o <exename> <objs> -Xlinker --start-group <libs> --end-group -lstdc++ -lz
#
mods_all:   -I <config>
libs_estel3d:/netdata/systel/LIBRARY/tecplot/tecplot_10/intel_64_10_C7/tecplot10.a
#MED libs_all: /netdata/systel/LIBRARY/MED/MED-3.0.5_gfortran_linux_C7/lib/libmed.a /netdata/systel/LIBRARY/HDF5/HDF5-1.8.7_gfortran_linux_C7/lib/libhdf5.a 
# _____               	              __________________________________
# ____/ Calibre7 GFORTRAN 4.4.5 MPICH _________________________________/
[C7.gfortranHPC]
#
options:    parallel mpi
#mpi_hosts: chap707 (example)
mpi_cmdexec: /usr/bin/mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    gfortran -c -O3 -fconvert=big-endian -frecord-marker=4 -DHAVE_MPI -DHAVE_VTK -DHAVE_TECPLOT <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    /usr/bin/mpif90 -fconvert=big-endian -frecord-marker=4 -lpthread -lm -o <exename> <objs> -Wl,--start-group <libs> -Wl,--end-group
cmd_exe_estel3d:    /usr/bin/mpif90 -fconvert=big-endian -frecord-marker=4 -lpthread -lm -o <exename> <objs> -Wl,--start-group <libs> -Wl,--end-group -lstdc++ -lz
#
mods_all:   -I <config>
libs_estel3d:/netdata/systel/LIBRARY/tecplot/tecplot_10/intel_64_10_C7/tecplot10.a
#
#
incs_parallel:      -I /usr/include/mpi/
libs_partel:      /netdata/systel/LIBRARY/metis/gfortran_linux_C7/libmetis.a
libs_all:	    -pthread -I/usr/lib/openmpi/lib -L/usr/lib/openmpi/lib -lmpi_f90 -lmpi_f77 -lmpi -lopen-rte -lopen-pal -ldl -Wl,--export-dynamic -lnsl -lutil -lm -ldl
#MED                /netdata/systel/LIBRARY/MED/MED-3.0.5_gfortran_linux_C7/lib/libmed.a /netdata/systel/LIBRARY/HDF5/HDF5-1.8.7_gfortran_linux_C7/lib/libhdf5.a 

# _____                     __________________________________
# ____/ Calibre7 INTEL MPICH _________________________________/
[C7.ifort10HPC]
#
options:    parallel mpi
#mpi_hosts: chap707 (example)
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    ifort -c -O3 -convert big_endian -DHAVE_VTK -DHAVE_TECPLOT -DHAVE_MPI <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    ifort -convert big_endian -o <exename> <objs> -Xlinker --start-group <libs> --end-group
cmd_exe_estel3d:    ifort -convert big_endian -o <exename> <objs> -Xlinker --start-group <libs> --end-group -lstdc++ -lz
#
mods_all:   -I <config>
libs_estel3d: /netdata/systel/LIBRARY/tecplot/tecplot_10/intel_64_10_C7/tecplot10.a
#
#
incs_parallel:      -I /netdata/systel/LIBRARY/mpi/intel_64_10/include
libs_partel:      /netdata/systel/LIBRARY/metis/intel_64_10_C7/libmetis.a
libs_all:           -L/netdata/systel/LIBRARY/mpi/intel_64_10/lib -lmpichf90 -lmpich -lopa -lpthread -lpthread -lrt
#MED                -L/netdata/systel/LIBRARY/MED/MED-3.0.5_intel_64_10_C7/lib -lmed -L/netdata/systel/LIBRARY/HDF5/HDF5-1.8.7_intel_64_10_C7/lib -lhdf5
# _____               	 __________________________________
# ____/ Calibre7 NAG 5.3 _________________________________/
[C7.nag5]
#
# no big Endian option... Please use the convert program in $HOMETEL/bin/convert
cmd_obj:    nagfor -c -O4 -w=obs -Oassumed -convert=BIG_ENDIAN -DHAVE_TECPLOT <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    nagfor -o <exename> <objs> -Wl,-Wl,,--start-group,<libsnag>-Wl,-Wl,,--end-group
cmd_exe_estel3d:    nagfor -o <exename> <objs> -Wl,--start-group <libs> -Wl,--end-group -lstdc++
#
mods_all:   -I <config>
libs_estel3d:/netdata/systel/LIBRARY/tecplot/tecplot_10/intel_64_10_C7/tecplot10.a
#
# _____               	       __________________________________
# ____/ Calibre7 NAG 5.3 MPICH _________________________________/
[C7.nag5HPC]
#
options:    parallel mpi
#mpi_hosts: chap707 (example)
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    nagfor -c -O4 -w=obs -Oassumed -convert=BIG_ENDIAN -DHAVE_TECPLOT -DHAVE_MPI <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    nagfor -o <exename> <objs> -Wl,-Wl,,--start-group,<libsnag>-Wl,-Wl,,--end-group
cmd_exe_esltel3d:    nagfor -o <exename> <objs> -Wl,--start-group <libs> -Wl,--end-group -lstdc++
#
mods_all:   -I <config>
libs_estel3d: /netdata/systel/LIBRARY/tecplot/tecplot_10/intel_64_10_C7/tecplot10.a
#
incs_parallel:      -I /netdata/systel/LIBRARY/mpi/NAG_64_5_C7/include
libs_partel:      /netdata/systel/LIBRARY/metis/gfortran_linux_C7/libmetis.a
libs_all:	    -L/netdata/systel/LIBRARY/mpi/NAG_64_5_C7/lib -lmpichf90 -lmpich -lopa -lpthread -lrt
#
# _____                        ____________________________________
# ____/ Edf ivanoe Cluster  /_____________________________________/
[ivanoe.ifort12]
#
root:       /home/projets/systel/V6P2
version:    v6p2
language:   2
modules:    update system
#
options:    parallel mpi hpc
mpi_cmdexec: mpiexec.hydra -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
hpc_stdin: #!/bin/bash
  #SBATCH --job-name=<jobname>
  #SBATCH --output=<jobname>-<time>.out
  #SBATCH --error=<jobname>-<time>.err
  #SBATCH --time=<walltime>
  #SBATCH --ntasks=<ncsize>
  #SBATCH --partition=<queue>
  ##SBATCH --exclude=cn[0000-0000,0000]
  ##SBATCH --exclusive
  srun hostname | sort > mpid.conf
  hosts=''
  for line in $(echo | cat mpid.conf)
  do
    hosts=$hosts:$line
  done 
  hosts=${hosts:1:${#hosts}}
  echo "$hosts" > test.txt
  rm -f mpitasks mpitasks.conf mpid.conf
  runcode.py <codename> <casfiles> <options> --hosts=$hosts
#
hpc_cmdexec: sbatch < <hpc_stdin>
#
cmd_obj:    ifort  -c -O3 -convert big_endian -no-prec-div -O3 -sox -vec-report1 -xsse4.2 -g -DHAVE_VTK -DHAVE_MPI -DHAVE_TECPLOT <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    ifort  -O3 -o <exename> <objs> -Xlinker --start-group <libs> --end-group
#
mods_all:   -I <config>
#
incs_parallel:      -I /logiciels/intel/impi/4.0.3.008/intel64/include
libs_partel:      /home/projets/systel/LIBRARY/metis/ivanoe_intel_openmpi_64_12/libmetis.V5.0.1.a
libs_all:           -L/logiciels/intel/impi/4.0.3.008/intel64/lib -lmpi -lmpigf -lpthread -lz -lstdc++ -v
#
sfx_zip:    .zip
sfx_lib:    .a
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:    
#
val_root:   <modpath>/validation
val_rank:   all
# _____                        ____________________________________
# ____/ IBM BluegeneQ cluster /___________________________________/
[zumbrota.xlf14]
#
root:       /home/projets_bgq/systel/V6P2
version:    v6p2
language:   2
modules:    update system
#
options:    parallel mpi hpc
#
par_cmdexec:   srun -n 1 <config>/partel < PARTEL.PAR >> <partel.log>
mpi_cmdexec: srun -n <ncsize> <exename>
#
hpc_stdin: #!/bin/bash
  #SBATCH --job-name=<jobname>
  #SBATCH --output=<jobname>-<time>.out
  #SBATCH --error=<jobname>-<time>.err
  #SBATCH --time=<walltime>
  #SBATCH --ntasks=<ncsize>
  #SBATCH --partition=<queue>
  ##SBATCH --exclude=cn[0000-0000,0000]
  ##SBATCH --exclusive
  runcode.py <codename> <casfiles> <options> 
#
hpc_cmdexec: sbatch < <hpc_stdin>
#
cmd_obj:    mpixlf90_r -fc=bgxlf_r -c -O2 -big-endian -qfullpath -qarch=qp -qstrict -qsimd=auto -qfixed -qnoescape -DHAVE_MPI -DHAVE_VTK -DHAVE_TECPLOT <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpixlf90_r -o <exename> <objs> -Wl,--start-group <libs> -Wl,--end-group
#
mods_all:   -I <config>
#
incs_parallel:      -I /bgsys/drivers/ppcfloor/comm/xl/include
libs_partel:      /home/B61570/LIBRARY/METIS/xl_12_zumbrota/lib/libmetis.a
libs_all:	    -L/bgsys/drivers/ppcfloor/comm/xl/lib -lmpichf90 -lmpich -lopa -lpthread -lrt 
#
sfx_zip:    .zip
sfx_lib:    .a
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:    
#
val_root:   <modpath>/validation
val_rank:   all

