###
### To add MPI support
###
#
# options: mpi
#
# cmd_obj: ... -DHAVE_MPI ...
#
# libs_partel: ... <path_to_metis>/lib/libmetis.a ...
#
# Replace in cmd_obj and cmd_exec the compiler by the mpi_compiler
#
###
### TO ADD VTK SUPPORT
###
# cmd_obj: ... -DHAVE_VTK ...
#
###
### TO ADD MED SUPPORT
###
# cmd_obj: ... -DHAVE_MED ...
#
# incs_all: ... -I<path-to-med>/include ...
# libs_all: ... -L<path-to-hdf5>/lib -L<path-to-med>/lib -lmed -lhdf5 -lstdc++ -lz ...
###
### TO ADD DREGEDIM SUPPORT
###
# cmd_obj: ... -DHAVE_DREDGESIM
# modules = system
#
#  !!! You will need to regenerate the cmdf files
#  !!! First remove all the cmdf
#  !!! On linux run the following command from the root directory
#  !!! find sources/ -iname *.cmdf -exec rm -vf {} +
#  !!! Then run compileTELEMAC.py --rescan
#
###
### TO ADD SCOTCH SUPPORT
###
#  !!! SCOTCH support still needs metis
# cmd_obj: ... -DHAVE_SCOTCH ...
# libs_partel: ... -L<path-to-scotch>/lib -lsctoch lsctocherr <path-to-metis>/lib/libmetis.a ...
#
###
### TO ADD PARMETIS SUPPORT
###
# cmd_obj: ... -DHAVE_PARMETIS ...
# libs_partel: ... <path-to-parmetis>/lib/libparmetis.a ...
#
###
### TO ADD PTSCOTCH SUPPORT
###
#  !!! PTSCOTCH support still needs parmetis
# cmd_obj: ... -DHAVE_PARMETIS -DHAVE_PTSCOTCH ...
# libs_partel: ... -L<path-to-ptscotch>/lib -lptsctoch lptsctocherr <path-to-parmetis>/lib/libparmetis.a ...
#
# _____                              _______________________________
# ____/ TELEMAC Project Definitions /______________________________/
#
[Configurations]
configs: C9.gfortranHPC.dyn
         winHPC winHPC.debug
         C9.gfortranHPC.debug C7.gfortranHPC.debug
         C9.nag5HPC C7.nag5HPC
         C9.ifort10HPC C7.ifort10HPC
         athos.intel14 athos.intel14.totalview
         zumbrota.xlf14
         porthos.intel14 porthos.intel14.totalview
         gcov
# _____                        ____________________________________
# ____/ General /___________________________________/
# Global declaration that are true for all the C7.* configuration
[general]
language:   2
modules:    system
#
sfx_zip:    .zip
sfx_lib:    .a
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:
#
#
val_root:   <root>/examples
#
val_rank:   all
#
mods_all:   -I <config>
#
incs_all: -I$MEDHOME/include
libs_all: -lm -L$MEDHOME/lib -lmed -L$HDF5HOME/lib -lhdf5 -lstdc++ -lz
          /data/projets/projets.002/systel.002/LIBRARY/metis/C7/libmetis.a
#
cmd_obj_c: gcc -c <srcName> -o <objName>

# _____               	              __________________________________
# ____/ Calibre7 GFORTRAN 4.4.5 MPICH _________________________________/
[C9.gfortranHPC.dyn]
#
sfx_lib:    .so
#
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    mpif90 -c -cpp -fPIC -O2 -fconvert=big-endian -frecord-marker=4 -DHAVE_MPI -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_lib:    mpif90 -g -fPIC -shared -fconvert=big-endian -frecord-marker=4 -lpthread -lm -o <libname> <objs>
cmd_exe:    mpif90 -g -fPIC -fconvert=big-endian -frecord-marker=4 -lpthread -lm -o <exename> <objs> <libs>
#
cmd_obj_c: gcc -c -fPIC <srcName> -o <objName>
# _____               	              __________________________________
# ____/ Calibre7 GFORTRAN 4.4.5 MPICH _________________________________/
[C7.gfortranHPC.debug]
#
options:    mpi
modules: system -mascaret
#
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    mpif90 -c -O0 -g -fbounds-check -fimplicit-none -Wall -fconvert=big-endian -frecord-marker=4 -DHAVE_MPI -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90 -fconvert=big-endian -g -frecord-marker=4 -lpthread -lm -o <exename> <objs> <libs>
# _____               	              __________________________________
# ____/ Calibre7 GFORTRAN 4.4.5 MPICH _________________________________/
[C9.gfortranHPC.debug]
#
options:    mpi
#
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    mpif90 -c -O0 -g -fcheck=all -fconvert=big-endian -frecord-marker=4 -DHAVE_MPI -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_obj_partel:    mpif90 -c -O0 -g -fcheck=array-temps,bounds,do,mem,recursion -fconvert=big-endian -frecord-marker=4 -DHAVE_MPI -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90 -g -fconvert=big-endian -frecord-marker=4 -lpthread -lm -o <exename> <objs> <libs>
# _____               	              __________________________________
# ____/ Calibre7 GFORTRAN 4.4.5 MPICH Code Coverage ___________________/
[gcov]
#
options:    mpi
#
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    mpif90 -c -O2 -fprofile-arcs -ftest-coverage -fPIC -fconvert=big-endian -frecord-marker=4 -DHAVE_MPI -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90 -fprofile-arcs -ftest-coverage -fPIC -fconvert=big-endian -frecord-marker=4 -lpthread -lm -o <exename> <objs> <libs>
# _____                     __________________________________
# ____/ Calibre7 INTEL MPICH _________________________________/
[C7.ifort10HPC]
#
options:    mpi
modules: system -mascaret
#
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    mpif90 -c -O3 -convert big_endian -DHAVE_VTK -DNO_INQUIRE_SIZE -DHAVE_MED -DHAVE_MPI <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90 -convert big_endian -o <exename> <objs> <libs>
# _____                     __________________________________
# ____/ Calibre7 INTEL MPICH _________________________________/
[C9.ifort10HPC]
#
options:    mpi
modules: system -mascaret
#
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    mpif90 -c -O2 -convert big_endian -DHAVE_VTK -DNO_INQUIRE_SIZE -DHAVE_MED -DHAVE_MPI <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90 -convert big_endian -o <exename> <objs> <libs>
#
# _____               	       __________________________________
# ____/ Calibre7 NAG 5.3 MPICH _________________________________/
[C7.nag5HPC]
#
options:    mpi
modules: system -mascaret
#
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    mpif90 -c -O0 -g -gline -w=obs -Oassumed -convert=BIG_ENDIAN -DHAVE_MED -DHAVE_MPI <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90 -g -gline -o <exename> <objs> <libs>
# _____               	       __________________________________
# ____/ Calibre7 NAG 5.3 MPICH _________________________________/
[C9.nag5HPC]
#
options:    mpi
modules: system -mascaret
#
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    mpif90 -c -O2 -w=obs -Oassumed -convert=BIG_ENDIAN -DNAGFOR -DHAVE_MED -DHAVE_MPI <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90 -o <exename> <objs> <libs>
# _____                        ____________________________________
# ____/ IBM BluegeneQ cluster /___________________________________/
[zumbrota.xlf14]
#
options:    mpi hpc
#
par_cmdexec: srun -n 1 <config>/partel < PARTEL.PAR >> <partel.log>
mpi_cmdexec: srun -n <ncsize> <exename>
#
hpc_stdin: #!/bin/bash
  #SBATCH --job-name=<jobname>
  #SBATCH --output=<jobname>-<time>.out
  #SBATCH --error=<jobname>-<time>.err
  #SBATCH --time=<walltime>
  #SBATCH --ntasks=<ncsize>
  #SBATCH --partition=<queue>
  ##SBATCH --exclude=cn[0000-0000,0000]
  ##SBATCH --exclusive
  srun hostname | sort > mpid.conf
  hosts=''
  for line in $(echo | cat mpid.conf)
  do
    hosts=$hosts:$line
  done
  hosts=${hosts:1:${#hosts}}
  echo "$hosts" > test.txt
  rm -f mpitasks mpitasks.conf mpid.conf
  source <root>/configs/pysource.<config>.sh
  <py_runcode>
#
hpc_runcode: cp HPC_STDIN ../;cd ../;sbatch < <hpc_stdin>
#
cmd_obj:    mpixlf90_r -fc=bgxlf_r -c -O3 -big-endian -qnoescape -WF,-DHAVE_MPI -WF,-DHAVE_MED -WF,-DHAVE_VTK <mods> <incs> <f95name>
cmd_obj_dredgesim:    mpixlf90_r -fc=bgxlf_r -c -O0 -big-endian -qnoescape -WF,-DHAVE_MPI -WF,-DHAVE_MED -WF,-DHAVE_VTK  <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpixlf90_r -o <exename> <objs> <libs>
#
incs_parallel:      -I /bgsys/drivers/ppcfloor/comm/xl/include
libs_partel:        /home/projets-bgq/systel/LIBRARY/metis-5.1.0/arch/zumbrota/lib/libmetis.a
libs_all:	    -L/bgsys/drivers/ppcfloor/comm/xl/lib -lmpichf90 -lmpich -lopa -lpthread -lrt
                    -L/home/projets-bgq/systel/LIBRARY/med-3.0.6/arch/zumbrota/lib -lmed
                    -L/home/projets-bgq/systel/LIBRARY/hdf5-1.8.8/arch/zumbrota/lib -lhdf5
                    /home/projets-bgq/systel/LIBRARY/tecplot/tecplot_10/BGQ_XLF_14/tecplot10.a
                    /bgsys/linux/RHEL6.3_V1R2M0-36/opt/ibmcmp/vacpp/bg/12.1/lib64/libibmc++.a
                    /bgsys/drivers/toolchain/V1R2M0-efix23/gnu-linux/powerpc64-bgq-linux/lib/libstdc++.a
                    /home/projets-bgq/systel/LIBRARY/zlib-1.2.7/arch/zumbrota/lib/libz.a

# _____                        ____________________________________
# ____/ Athos cluster /___________________________________/
[athos.intel14]
#
options:    mpi hpc
par_cmdexec: srun -n 1 -N 1 <config>/partel < PARTEL.PAR >> <partel.log>
mpi_cmdexec: mpirun -np <ncsize> <exename>
#
hpc_stdin: #!/bin/bash
  #SBATCH --job-name=<jobname>
  #SBATCH --output=<jobname>-<time>.out
  #SBATCH --error=<jobname>-<time>.err
  #SBATCH --time=<walltime>
  #SBATCH --ntasks=<ncsize>
  #SBATCH --partition=<queue>
  ##SBATCH --exclude=cn[0000-0000,0000]
  #SBATCH --exclusive
  #SBATCH --nodes=<ncnode>
  #SBATCH --ntasks-per-node=<nctile>
  source <root>/configs/pysource.<configName>.sh
  <py_runcode>
#
hpc_runcode: cp HPC_STDIN ../;cd ../;sbatch < <hpc_stdin>
#
cmd_obj:    mpif90  -c -convert big_endian -O3 -DHAVE_MPI -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90  -o <exename> <objs> <libs>
#
libs_partel:     /home/projets/systel/V6P3R2/optionals/metis-5.1.0/libmetis/libmetis.a

[athos.intel14.totalview]
#
options:    mpi hpc
par_cmdexec: srun -n 1 -N 1 <config>/partel < PARTEL.PAR >> <partel.log>
mpi_cmdexec: mpirun -np <ncsize> <exename>
#
hpc_stdin: #!/bin/bash
  #SBATCH --job-name=<jobname>
  #SBATCH --output=<jobname>-<time>.out
  #SBATCH --error=<jobname>-<time>.err
  #SBATCH --time=<walltime>
  #SBATCH --ntasks=<ncsize>
  #SBATCH --partition=<queue>
  ##SBATCH --exclude=cn[0000-0000,0000]
  #SBATCH --exclusive
  #SBATCH --nodes=<ncnode>
  #SBATCH --ntasks-per-node=<nctile>
  source <root>/configs/pysource.<configName>.sh
  <py_runcode>
#
hpc_runcode: cp HPC_STDIN ../;cd ../;sbatch < <hpc_stdin>
#
cmd_obj:    mpif90  -c -convert big_endian -debug all -check all -traceback -ftrapuv -O0 -g -DHAVE_MPI -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90  -g -debug all -check all -traceback -ftrapuv -o <exename> <objs> <libs>
# Removing -check all for Partel because option is not compatible with iso_c_binding
cmd_obj_partel:    mpif90  -c -convert big_endian -debug all -traceback -ftrapuv -O0 -g -DHAVE_MPI -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_exe_partel:    mpif90  -g -debug all -traceback -ftrapuv -o <exename> <objs> <libs>
#
libs_partel:     /home/projets/systel/V6P3R2/optionals/metis-5.1.0/libmetis/libmetis.a
# _____                        ____________________________________
# ____/ Porthos cluster /___________________________________/
[porthos.intel14]
#
options:    mpi hpc
par_cmdexec: srun -n 1 -N 1 <config>/partel < PARTEL.PAR >> <partel.log>
mpi_cmdexec: mpirun -np <ncsize> <exename>
#
hpc_stdin: #!/bin/bash
  #SBATCH --job-name=<jobname>
  #SBATCH --output=<jobname>-<time>.out
  #SBATCH --error=<jobname>-<time>.err
  #SBATCH --time=<walltime>
  #SBATCH --ntasks=<ncsize>
  #SBATCH --partition=<queue>
  ##SBATCH --exclude=cn[0000-0000,0000]
  #SBATCH --exclusive
  #SBATCH --nodes=<ncnode>
  #SBATCH --ntasks-per-node=<nctile>
  source <root>/configs/pysource.<configName>.sh
  <py_runcode>
#
hpc_runcode: cp HPC_STDIN ../;cd ..;sbatch < <hpc_stdin>
#
cmd_obj:    mpif90  -c -convert big_endian -O3 -DHAVE_MPI -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90  -o <exename> <objs> <libs>
#
libs_all: -lm $MEDHOME/lib/libmed.a -L$HDF5HOME/lib -lhdf5 -lstdc++ /lib/x86_64-linux-gnu/libz.so.1
          /home/projets/systel/V7P0/optionals/metis-5.1.0/libmetis/libmetis.a

[porthos.intel14.totalview]
#
options:    mpi hpc
par_cmdexec: srun -n 1 -N 1 <config>/partel < PARTEL.PAR >> <partel.log>
mpi_cmdexec: mpirun -np <ncsize> <exename>
#
hpc_stdin: #!/bin/bash
  #SBATCH --job-name=<jobname>
  #SBATCH --output=<jobname>-<time>.out
  #SBATCH --error=<jobname>-<time>.err
  #SBATCH --time=<walltime>
  #SBATCH --ntasks=<ncsize>
  #SBATCH --partition=<queue>
  ##SBATCH --exclude=cn[0000-0000,0000]
  #SBATCH --exclusive
  #SBATCH --nodes=<ncnode>
  #SBATCH --ntasks-per-node=<nctile>
  source <root>/configs/pysource.<configName>.sh
  <py_runcode>
#
hpc_runcode: cp HPC_STDIN ../;cd ..;sbatch < <hpc_stdin>
#
cmd_obj:    mpif90  -c -convert big_endian -debug all -check all -traceback -ftrapuv -O0 -g -DHAVE_MPI -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90  -g -debug all -check all -traceback -ftrapuv -o <exename> <objs> <libs>
# Removing -check all for Partel because option is not compatible with iso_c_binding
cmd_obj_partel:    mpif90  -c -convert big_endian -debug all -traceback -ftrapuv -O0 -g -DHAVE_MPI -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_exe_partel:    mpif90  -g -debug all -traceback -ftrapuv -o <exename> <objs> <libs>
#
libs_all: -lm $MEDHOME/lib/libmed.a -L$HDF5HOME/lib -lhdf5 -lstdc++ /lib/x86_64-linux-gnu/libz.so.1
          /home/projets/systel/V7P0/optionals/metis-5.1.0/libmetis/libmetis.a

# _____                              _______________________________
# ____/ windows 7 gfortran parallel /______________________________/
[winHPC]
#
cmd_obj:    gfortran -c -O2 -DHAVE_MPI -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    gfortran -fconvert=big-endian -frecord-marker=4 -lm -o <exename> <objs> <libs>
#
mpi_cmdexec:   mpiexec.exe -wdir <wdir> -n <ncsize> <exename>
#
incs_all:
#
sfx_lib:    .lib
sfx_exe:    .exe
#
incs_all:      -I <root>\..\mpich2\include
libs_all:      <root>\..\mpich2\lib\libfmpich2g.a
               <root>\..\metis\lib\libmetis.a

[winHPC.debug]
#
cmd_obj:    gfortran -c -Wall -g -O0 -DHAVE_MPI -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    gfortran -g -fconvert=big-endian -frecord-marker=4 -lm -o <exename> <objs> <libs>
#
mpi_cmdexec:   mpiexec.exe -wdir <wdir> -n <ncsize> <exename>
#
incs_all:
#
sfx_lib:    .lib
sfx_exe:    .exe
#
incs_all:      -I <root>\..\mpich2\include
libs_all:      <root>\..\mpich2\lib\libfmpich2g.a
               <root>\..\metis\lib\libmetis.a
