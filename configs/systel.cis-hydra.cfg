# _____                              _______________________________
# ____/ TELEMAC Project Definitions /______________________________/
#
[Configurations]
configs:    hydra_gcc_s  
# ubugfopenmpi ubugfopenmpidbg

# _____                _____________________________________________
# ____/ HYDRA gcc 4.6 /____________________________________________/

[hydra_gcc_s]
#
root:       SetByCIS 
version:    SetByCIS
language:   2
modules:    SetByCIS
options:    
#
mpi_hosts:    mgmt01
# -w option in runTELEMAC.py gives value for wdir
mpi_cmdexec: /apps/openmpi/1.6/gcc/4.4.6/bin/mpiexec -wdir <wdir> -n 1 <exename> --hostfile <hostfile>
#
par_cmdexec:   
#<config>/partel  < PARTEL.PAR >> <partel.log>
#par_cmdexec:   <config>/partel_prelim; <mpi_cmdexec> <config>/partel_para >> <partel.log>
#par_cmdexec:   <config>/partel_prelim; python <root>/pytel/utils/partitioning.py
# >> <partel.log>
#
hpc_stdin: #!/bin/bash
   #PBS -S /bin/sh
   # stdout file
   #PBS -o <sortiefile>    
   # stderr file   
   #PBS -e <exename>.err    
   # name (-j option in runTELEMAC.py)
   #PBS -N <jobname>        
   # nodes required / processors per node
   #PBS -l nodes=1:ppn=1    
   # queue name
   #PBS -q serialq          
   <mpi_cmdexec>
   echo ' My HPC is done ' > <wdir>/<jobname>.done
   exit
#
hpc_cmdexec:   chmod 755 <hpc_stdin>; qsub <hpc_stdin>
#
cmd_obj:    gfortran -c -O3 -ffixed-line-length-132 -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90 -fconvert=big-endian -frecord-marker=4 -v -lm -lz -o <exename> <objs> <libs>
#
mods_all:   -I <config>
#
incs_parallel:      -I /apps/openmpi/1.6/gcc/4.4.6/include/
libs_parallel:      /home/HR/sbo/opentelemac/libs/libmetis.a
libs_all:           /apps/openmpi/1.6/gcc/4.4.6/lib/libmpi.so
#
sfx_zip:    .gztar
sfx_lib:    .lib
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:








[hydra_gcc_p]
#
root:       SetByCIS 
version:    SetByCIS
language:   2
modules:    SetByCIS
options:    parallel mpi hpc
#
mpi_hosts:    mgmt01
mpi_cmdexec: /apps/openmpi/1.6/gcc/4.4.6/bin/mpiexec -wdir <wdir> -n <ncsize> <exename> --hostfile <hostfile>
#
par_cmdexec:   <config>/partel  < PARTEL.PAR >> <partel.log>
#par_cmdexec:   <config>/partel_prelim; <mpi_cmdexec> <config>/partel_para >> <partel.log>
#par_cmdexec:   <config>/partel_prelim; python <root>/pytel/utils/partitioning.py
# >> <partel.log>
#
hpc_stdin: #!/bin/bash
   #PBS -S /bin/sh
   #PBS -o <sortiefile>
   #PBS -e <exename>.err
   #PBS -N <jobname>
   #PBS -l nodes=2:ppn=10
   #PBS -q highp
   <mpi_cmdexec>
   source /etc/profile.d/modules.sh
   module load gcc/4.4.6 openmpi/1.6/gcc/4.4.6 python/2.7.2
   cd <wdir>;/data/home/HR/sbo/opentelemac/eticsm/pytel/ETI/FFS/slf_utilities.py crunch --vars 'DENS[192:1582],SURF[384:576],MID[1180:1229]' T2DRES T2DVAR --ncsize 20
   echo ' My csm is done ' > <wdir>/<jobname>.done
   exit
#
hpc_cmdexec:   chmod 755 <hpc_stdin>; qsub <hpc_stdin>
#
cmd_obj:    gfortran -c -O3 -ffixed-line-length-132 -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90 -fconvert=big-endian -frecord-marker=4 -v -lm -lz -o <exename> <objs> <libs>
#
mods_all:   -I <config>
#
incs_parallel:      -I /apps/openmpi/1.6/gcc/4.4.6/include/
libs_parallel:      /home/HR/sbo/opentelemac/libs/libmetis.a
libs_all:           /apps/openmpi/1.6/gcc/4.4.6/lib/libmpi.so
#
sfx_zip:    .gztar
sfx_lib:    .lib
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:



# _____                         ____________________________________
# ____/ Hydra gfortran scalar /___________________________________/
[hydra_gfortran_s]
#
root:       /home/HR/otm_cis/workspace/trunk
version:    v6p2
language:   2
modules:    update system
options:
#
cmd_obj:    gfortran -c -O3 -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    gfortran -fconvert=big-endian -frecord-marker=4 -v -o <exename> <objs> <libs>
#
mods_all:   -I <config>
#
sfx_zip:    .gztar
sfx_lib:    .lib
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:
#
# also possible val_root:   <root>/../validation
val_root:   <root>/examples
val_rank:   all
# also possible val_rank:   <3 >7 6
#
# _____                               ____________________________________
# ____/ Ubuntu gfortran scalar debug/___________________________________/
[ubugfortransdbg]
#
root:       /home/telemac/workspace/trunk
version:    v6p2
language:   2
modules:    update system
options:
#
cmd_obj:    gfortran -c -g -fbounds-check -Wall -fbacktrace -finit-real=nan -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    gfortran -fconvert=big-endian -frecord-marker=4 -v -o <exename> <objs> <libs>
#
mods_all:   -I <config>
#
sfx_zip:    .gztar
sfx_lib:    .lib
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:
#
# also possible val_root:   <root>/../validation
val_root:   <root>/examples
val_rank:   all
# also possible val_rank:   <3 >7 6
#
# _____                          ___________________________________
# ____/ Ubuntu gfortran openMPI /__________________________________/
[ubugfopenmpi]
#
root:       /home/telemac/workspace/trunk
version:    v6p2
language:   2
modules:    update system
#
options:    parallel mpi
#
par_cmdexec:   <config>/partel < PARTEL.PAR >> <partel.log>
#
mpi_cmdexec:   /usr/bin/mpiexec -wdir <wdir> -n <ncsize> <exename>
mpi_hosts:
#
cmd_obj:    gfortran -c -O3 -fbounds-check -Wall -fbacktrace -finit-real=nan -DHAVE_MPI -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    /usr/bin/mpif90 -fconvert=big-endian -frecord-marker=4 -lpthread -v -lm -o <exename> <objs>  <libs>
#
mods_all:   -I <config>
#
incs_parallel:      -I /usr/lib/openmpi/include/
libs_partel:      /home/telemac/metis-5.0.2/build/Linux-x86_64/libmetis/libmetis.a
libs_all       :    /usr/lib/openmpi/lib/libmpi.so
#
sfx_zip:    .gztar
sfx_lib:    .lib
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:
#
# also possible val_root:   <root>/../validation
val_root:   <root>/examples
val_rank:   all
# also possible val_rank:   <3 >7 6
#
# _____                                ___________________________________
# ____/ Ubuntu gfortran openMPI debug /__________________________________/
[ubugfopenmpidbg]
#
root:       /home/telemac/workspace/trunk
version:    v6p2
language:   2
modules:    update system
#
options:    parallel mpi
#
par_cmdexec:   <config>/partel < PARTEL.PAR >> <partel.log>
#
mpi_cmdexec:   /usr/bin/mpiexec -wdir <wdir> -n <ncsize> <exename>
mpi_hosts:
#
cmd_obj:    gfortran -c -g -fbounds-check -Wall -fbacktrace -finit-real=nan -DHAVE_MPI -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    /usr/bin/mpif90 -fconvert=big-endian -frecord-marker=4 -lpthread -v -lm -o <exename> <objs>  <libs>
#
mods_all:   -I <config>
#
incs_parallel:      -I /usr/lib/openmpi/include/
libs_partel:      /home/telemac/metis-5.0.2/build/Linux-x86_64/libmetis/libmetis.a
libs_all       :    /usr/lib/openmpi/lib/libmpi.so
#
sfx_zip:    .gztar
sfx_lib:    .lib
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:
#
# also possible val_root:   <root>/../validation
val_root:   <root>/examples
val_rank:   all
# also possible val_rank:   <3 >7 6
#
