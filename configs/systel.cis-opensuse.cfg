# _____                              _______________________________
# ____/ TELEMAC Project Definitions /______________________________/
#
[Configurations]
#
configs:  susgfortransdbg susgfopenmpidbg susgfortrans susgfopenmpi
#
#
#
# _____          ___________________________________________________
# ____/ GENERAL /__________________________________________________/
[general]
modules:    system -dredgesim
#
cmd_lib:    ar cru <libname> <objs>
#
mods_all:   -I <config>
#
sfx_zip:    .gztar
sfx_lib:    .a
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:
#
val_root:   <root>/examples
val_rank:   all
# also possible val_rank:   <3 >7 6
# _____                           __________________________________
# ____/ OpenSUSE gfortran scalar /_________________________________/
[susgfortrans]
#
brief: scalar mode, Fortran optimisation 3.
   TELEMAC will work whether processor is 0 or 1
#
cmd_obj:    gfortran -c -O3 -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    gfortran -fconvert=big-endian -frecord-marker=4 -v -lm -lz -o <exename> <objs> <libs>
#
# _____                                 __________________________________
# ____/ OpenSUSE gfortran scalar debug /_________________________________/
[susgfortransdbg]
#
brief: scalar mode, Fortran debug mode.
   TELEMAC will work whether processor is 0 or 1
#
cmd_obj:    gfortran -c -g -fbounds-check -Wall -fbacktrace -finit-real=nan -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    gfortran -fconvert=big-endian -frecord-marker=4 -v -lm -lz -o <exename> <objs> <libs>
#
# _____                            _________________________________
# ____/ OpenSUSE gfortran openMPI /________________________________/
[susgfopenmpi]
#
brief: parallel mode, using mpiexec directly (of the openMPI package).
       The only difference with the scalar versions (optimised) is the presence
       of the key mpi_cmdexec and the -DHAVE_MPI compilation directive.
       Of course, you also need the key par_cmdexec.
       Finally, note that this configuration also works whether
       processor is 0 or 1.
#
mpi_cmdexec:   /usr/lib64/mpi/gcc/openmpi/bin/mpiexec -wdir <wdir> -n <ncsize> --default-hostfile none <exename>
#
cmd_obj:    gfortran -c -O3 -DHAVE_MPI -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    /usr/lib64/mpi/gcc/openmpi/bin/mpif90 -fconvert=big-endian -frecord-marker=4 -v -lm -lz -o <exename> <objs> <libs>
#
incs_special:       -I /usr/lib64/mpi/gcc/openmpi/include
incs_parallel:      -I /usr/lib64/mpi/gcc/openmpi/include
libs_partel:        /home/telemac/metis-5.0.2/build/Linux-x86_64/libmetis/libmetis.a
libs_all:           /usr/lib64/mpi/gcc/openmpi/lib64/libmpi.so
#
# _____                                  _________________________________
# ____/ OpenSUSE gfortran openMPI debug /________________________________/
[susgfopenmpidbg]
#
brief: parallel mode, using mpiexec directly (of the openMPI package).
       The only difference with the scalar versions (debugged) is the presence
       of the key mpi_cmdexec and the -DHAVE_MPI compilation directive.
       Of course, you also need the key par_cmdexec.
       Finally, note that this configuration also works whether
       processor is 0 or 1.
#
mpi_cmdexec:   /usr/lib64/mpi/gcc/openmpi/bin/mpiexec -wdir <wdir> -n <ncsize> --default-hostfile none <exename>
#
cmd_obj:    gfortran -c -g -fbounds-check -Wall -fbacktrace -finit-real=nan -DHAVE_MPI -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    /usr/lib64/mpi/gcc/openmpi/bin/mpif90 -fconvert=big-endian -frecord-marker=4 -v -lm -lz -o <exename> <objs> <libs>
#
incs_special:       -I /usr/lib64/mpi/gcc/openmpi/include
incs_parallel:      -I /usr/lib64/mpi/gcc/openmpi/include
libs_partel:        /home/telemac/metis-5.0.2/build/Linux-x86_64/libmetis/libmetis.a
libs_all:           /usr/lib64/mpi/gcc/openmpi/lib64/libmpi.so
#
